{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
      "metadata": {},
      "source": [
        "# Simple RAG\n",
        "\n",
        "This notebook will show you how to build a simple RAG application with Llama Stack. You will learn how the API's provided by Llama Stack can be used to directly control and invoke all common RAG stages, including indexing, retrieval and inference.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers the following steps:\n",
        "1. Indexing a collection of documents into a vector database for later retrieval.\n",
        "2. Executing the built-in RAG tool to retrieve the document chunks relevant to a given query.\n",
        "3. Using the retrieved context to answer user queries during the inference step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
      "metadata": {},
      "source": [
        "## 1. Setting Up this Notebook\n",
        "\n",
        "First of all, we need to install the correct version of the Llama Stack client which we will use for this use case. This needs to match the version of the Llama Stack server which is part of OpenShift AI, in this case: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2931a30a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama_stack_client==0.2.10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d80bff1",
      "metadata": {},
      "source": [
        "Then, we will need a few imports we use throughout the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15080a6-48be-4475-8813-c584701d69bf",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_stack_client import RAGDocument, LlamaStackClient\n",
        "\n",
        "# Just used for better readability of std output\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46eff757",
      "metadata": {},
      "source": [
        "Now we initialize the Llama Stack client itself. We only need to provide the endpoint it's reachable with. This is exposed via a Kubernetes Service on the underlying cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c72a59",
      "metadata": {},
      "outputs": [],
      "source": [
        "client = LlamaStackClient(base_url=\"http://lsd-llama-milvus-service.llamastack.svc.cluster.local:8321\")\n",
        "\n",
        "print(f\"Connected to Llama Stack server\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
      "metadata": {},
      "source": [
        "Next, we want to check which models are available to us and set the `model_id` to use later on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch all registered models\n",
        "models = client.models.list()\n",
        "\n",
        "# Let's see what's in there\n",
        "print('Following models are available:')\n",
        "\n",
        "models\n",
        "\n",
        "# Set the `model_id`to the one which is a \"llm\"\n",
        "model_id = next(m.identifier for m in models if m.model_type == \"llm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
      "metadata": {},
      "source": [
        "Finally, we complete the setup by initializing the document collection we will use for RAG ingestion and retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c725c2da-05e5-474f-9a44-cf5615557665",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f98a94",
      "metadata": {},
      "source": [
        "# UPDATED UNTIL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
      "metadata": {},
      "source": [
        "## 2. Indexing the Documents\n",
        "- Initialize a new document collection in our vector database. All parameters related to the vector database, such as the embedding model and dimension, must be specified here.\n",
        "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, converting, and chunking the content of the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define and register the document collection to be used\n",
        "client.vector_dbs.register(\n",
        "    vector_db_id=vector_db_id,\n",
        "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
        "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
        "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
        ")\n",
        "\n",
        "# ingest the documents into the newly created document collection\n",
        "urls = [\n",
        "    (\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/pdf/architecture/OpenShift_Container_Platform-4.19-Architecture-en-US.pdf\", \"application/pdf\"),\n",
        "]\n",
        "documents = [\n",
        "    RAGDocument(\n",
        "        document_id=f\"num-{i}\",\n",
        "        content=url,\n",
        "        mime_type=url_type,\n",
        "        metadata={},\n",
        "    )\n",
        "    for i, (url, url_type) in enumerate(urls)\n",
        "]\n",
        "client.tool_runtime.rag_tool.insert(\n",
        "    documents=documents,\n",
        "    vector_db_id=vector_db_id,\n",
        "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
      "metadata": {},
      "source": [
        "## 3. Executing Queries via the Built-in RAG Tool\n",
        "- Directly invoke the RAG tool to query the vector database we ingested into at the previous stage.\n",
        "- Construct an extended prompt using the retrieved chunks.\n",
        "- Query the model with the extended prompt.\n",
        "- Output the reply received from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"How do I install OpenShift?\",\n",
        "]\n",
        "\n",
        "for prompt in queries:\n",
        "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
        "    \n",
        "    # RAG retrieval call\n",
        "    rag_response = client.tool_runtime.rag_tool.query(content=prompt, vector_db_ids=[vector_db_id])\n",
        "\n",
        "    # the list of messages to be sent to the model must start with the system prompt\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
        "    prompt_context = rag_response.content\n",
        "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
        "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
        "\n",
        "    # use Llama Stack inference API to directly communicate with the desired model\n",
        "    response = client.inference.chat_completion(\n",
        "        messages=messages,\n",
        "        model_id=model_id,\n",
        "        sampling_params=sampling_params,\n",
        "        stream=stream,\n",
        "    )\n",
        "    \n",
        "    # print the response\n",
        "    cprint(\"inference> \", color=\"magenta\", end='')\n",
        "    if stream:\n",
        "        for chunk in response:\n",
        "            response_delta = chunk.event.delta\n",
        "            if isinstance(response_delta, TextDelta):\n",
        "                cprint(response_delta.text, color=\"magenta\", end='')\n",
        "            elif isinstance(response_delta, ToolCallDelta):\n",
        "                cprint(response_delta.tool_call, color=\"magenta\", end='')\n",
        "    else:\n",
        "        cprint(response.completion_message.content, color=\"magenta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
        "\n",
        "Now that we've seen how easy it is to implement RAG with Llama Stack, We'll move on to building a simple agent with Llama Stack next in our [Simple Agents](./Level2_simple_agent_with_websearch.ipynb) notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb5b323",
      "metadata": {},
      "source": [
        "#### Any Feedback?\n",
        "\n",
        "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
