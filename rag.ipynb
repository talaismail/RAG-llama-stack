{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
      "metadata": {},
      "source": [
        "# Simple Retrieval Augmented Generation (RAG)\n",
        "\n",
        "This notebook will show you how to build a simple RAG application with Llama Stack. You will learn how the API's provided by Llama Stack can be used to directly control and invoke all common RAG stages, including indexing, retrieval and inference.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers the following steps:\n",
        "1. Setting up the Llama Stack client\n",
        "2. Creating and reusing an agent for a conversation like approach with our LLM\n",
        "3. Indexing a collection of documents into a vector database for later retrieval.\n",
        "4. Executing the built-in RAG tool to retrieve the document chunks relevant to a given query.\n",
        "5. Using the retrieved context to answer user queries during the inference step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
      "metadata": {},
      "source": [
        "## 1. Setting Up this Notebook\n",
        "\n",
        "First of all, we need to install the Llama Stack client which we will use for this use case. The Llama Stack server has already been deployed as part of OpenShift AI so we only need to get the client and connect as well as use the provided API's later: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2931a30a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama_stack_client"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d80bff1",
      "metadata": {},
      "source": [
        "Then, we will need a few imports we use throughout the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15080a6-48be-4475-8813-c584701d69bf",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "import io\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46eff757",
      "metadata": {},
      "source": [
        "Now we initialize the Llama Stack client itself. We only need to provide the endpoint it's reachable with. This is exposed via a Kubernetes Service on the underlying cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c72a59",
      "metadata": {},
      "outputs": [],
      "source": [
        "client = LlamaStackClient(base_url='http://lsd-llama-milvus-service.llamastack.svc.cluster.local:8321')\n",
        "\n",
        "print('Connected to Llama Stack server')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
      "metadata": {},
      "source": [
        "Next, we want to check which models are available to us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch all registered models\n",
        "models = client.models.list()\n",
        "\n",
        "# Let's see what's in there\n",
        "print('Following models are available:')\n",
        "print(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0e705b",
      "metadata": {},
      "source": [
        "Taking a look at the `model_type` of our models, we can see that we have an LLM available (`llama-3-2-3b`) as well as two embedding models. The LLM is used for the actual inferencing later, digesting your queries / messages and answering based on trained information and later even with additional information you provided via RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
      "metadata": {},
      "source": [
        "For now let's set the model vars for our LLM and embedding model for later usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c725c2da-05e5-474f-9a44-cf5615557665",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# We select the first llm available model - in this case we only have one\n",
        "model_id = next(m.identifier for m in models if m.model_type == \"llm\")\n",
        "\n",
        "# Also selecting the first available embedding model, we have 2 but any works\n",
        "embedding_model = next(m for m in models if m.model_type == \"embedding\")\n",
        "embedding_model_id = embedding_model.identifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8ea0a1",
      "metadata": {},
      "source": [
        "A quick check to see if we have set the correct models, should be `vllm-inference/llama-3-2-3b` and `granite-embedding-125m`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a14ff63",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model_id)\n",
        "print(embedding_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c30977f",
      "metadata": {},
      "source": [
        "Great so far! Let's check now what kind of providers are available from our Llama Stack server. These providers are basically capabilities which are (partially) implemented:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1404243",
      "metadata": {},
      "outputs": [],
      "source": [
        "for provider in client.providers.list():\n",
        "    print(provider.provider_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceac510a",
      "metadata": {},
      "source": [
        "There are quite some, right?! Some are self explanatory, like `llm-inference` which is the basic capability of querying the configured LLM or `sentence-transformers` (which is used by the embedding model) to digest text for our actual RAG use case. The Llama Stack can do quite some stuff, but we will focus on the basics for now. If you are eager to do more, feel free to reach out to us!\n",
        "\n",
        "An important provider is `milvus`. This is our vector database of choice here, as it is integrated into the server side Llama Stack via OpenShift AI. This means we can already interact with a vector database to store and _*retrieve*_ (as in RAG). We only need to create our own store in the database and use it with our Llama Stack client. To do so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c93a90a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your unique ID for your own vector store containing documents for the RAG use case\n",
        "vector_store_name = f'vector_store_{uuid4()}'\n",
        "\n",
        "# Set the milvus id, which is the one listed in our earlier step\n",
        "provider_id = 'milvus'\n",
        "\n",
        "# Initialize and set our own store\n",
        "vector_store = client.vector_stores.create(\n",
        "    name=vector_store_name,\n",
        "    extra_body={\n",
        "        \"provider_id\": provider_id,\n",
        "        \"embedding_model\": embedding_model_id,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"Registered vector Store: {vector_store_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6d3bd5",
      "metadata": {},
      "source": [
        "So let's check now what your vector store looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5a6160",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbb4ca1",
      "metadata": {},
      "source": [
        "You should see a section `FileCounts( ... )` where you can see, that there no files have been ingested so far (I mean that makes sense since we literally just created the store). Let's check that later once we ingested something!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "664f79c3",
      "metadata": {},
      "source": [
        "### Status\n",
        "\n",
        "Congrats! If you made it this far, you should have your own vector store instance in the Milvus VectorDB and configured your Llama Stack client and Jupyter notebook environment to leverage the necessary basics:\n",
        "- An LLM for the inferencing part\n",
        "- An embedding model for the ingestion of documents\n",
        "- Your own vector store to ingest your documents for retrieval\n",
        "- The Llama Stack client itself to do all that with simplified functions\n",
        "\n",
        "In the next step we will start to query the LLM via the Llama Stack and prepare the rest for the actual retrieval part!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
      "metadata": {},
      "source": [
        "## 2. Let's get querying\n",
        "Now comes the fun part: we start to interact the LLM through the Llama Stack. The underlying LLM is quite small and has limited information stored. We will start by preparing everything to be able to interact with an agent to ultimately talk with our LLM. Once we did that, in step 3 we will start adding new information for the actual RAG use case.\n",
        "\n",
        "So let's get going!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12fc691d",
      "metadata": {},
      "source": [
        "First of all, let's check the capabilities our Llama Stack server has:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07472ae5",
      "metadata": {},
      "outputs": [],
      "source": [
        "client.tools.list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2558c0dc",
      "metadata": {},
      "source": [
        "Our stack comes with 3 built-in tools: `web_search`, `insert_into_memory` and `knowledge_search`, let's drill them down:\n",
        "- `web_search`: as the name indicates, triggers a web search by the Llama Stack server to collect additional information for a given request\n",
        "- `insert_into_memory`: allows for ephemeral knowledge, is for example used internally after doing a web search to put this information in there for a session to serve as a form of memory\n",
        "- `knowledge_search`: will retrieve information available in provided vector stores - we will use this capability for our use case here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a79665a6",
      "metadata": {},
      "source": [
        "To interact with our LLM through the Llama Stack client, we can leverage two API's, the `Response` and the `Agent` one. The `Response` is perfect for \"one off\" / stateless queries where we just want to query once while `Agent` is a great wrapper to maintain a stateful conversation. Let's go with the `Agent` one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6d7227",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the list of tools we want to leverage in our agent.\n",
        "# We can make use of the tools provided out of the box by Llama Stack\n",
        "# (as seen by the list tools call before) and also custom ones.\n",
        "# Here we go with the file search tool which is the one to search\n",
        "# in vector stores (our RAG knowledge base in this case).\n",
        "#\n",
        "# NOTE: you might be wondering why we call it file_search and not\n",
        "# knowledge_search as mentioned before. The reason is that the Llama Stack\n",
        "# API maintains OpenAI compatibility where it's called like this and\n",
        "# it will be mapped accordingly on the server side.\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"file_search\",\n",
        "        \"vector_store_ids\": [vector_store.id],\n",
        "    }\n",
        "]\n",
        "\n",
        "# These instructions are very important to guide the agent behavior\n",
        "# towards the desired RAG use case. Here we explicitly tell it to\n",
        "# ALWAYS use the knowledge_search tool. We do so by providing clear\n",
        "# instructions on how to behave - that's just how LLMs work best!\n",
        "instructions = \"\"\"You are a helpful AI assistant with access to a knowledge base via the knowledge_search tool\n",
        "When answering questions:\n",
        "1. ALWAYS use the knowledge_search tool first\n",
        "2. Provide specific details from the documentation\n",
        "3. If information isn't in the knowledge base, say so clearly\n",
        "4. Be concise but thorough\n",
        "\"\"\"\n",
        "\n",
        "agent = Agent(\n",
        "    client=client,\n",
        "    model=model_id,\n",
        "    instructions=instructions,\n",
        "    tools=tools,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc2a293",
      "metadata": {},
      "source": [
        "With the agent instantiated, we can use it to have a conversation now! As stated before, it is stateful, the statefulness comes from a unique session ID we provide. Every subsequent invocation with this ID will leverage this.\n",
        "\n",
        "*Important: if you want to start a new conversation rerun the session ID generation here to create a new unique one to use!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb466f85",
      "metadata": {},
      "outputs": [],
      "source": [
        "session_id = agent.create_session(f\"interactive-{uuid4().hex[:8]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a45b16",
      "metadata": {},
      "source": [
        "Now your actual message you want to provide to the Llama Stack / model and the callback to our agent to start our conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0227854b",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_input = \"How are you doing?\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"type\": \"message\",\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"input_text\", \"text\": user_input}],\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\" Assistant:\", end=\" \", flush=True)\n",
        "\n",
        "# Stream response with event logging\n",
        "event_logger = AgentEventLogger()\n",
        "\n",
        "for chunk in agent.create_turn(messages=messages, session_id=session_id, stream=True):\n",
        "    for log_msg in event_logger.log([chunk]):\n",
        "        print(log_msg, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38536571",
      "metadata": {},
      "source": [
        "As you can see from the output, the `file_search` tool has been used for this query. For the message / question we used this was not really necessary as it added no new knowledge (and we don't even have any documents to retrieve so far) but we stated in the instructions to *ALWAYS* use the `file_search` first, so it's performing as intended! You can already play around with the message above and see what the responses are."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe18eec3",
      "metadata": {},
      "source": [
        "### Status\n",
        "Now we are very close for the actual RAG part. We have a working agent to talk to in a stateful, conversation like manner and learnt a bit about the API and the possibilities we have. So far we haven't ingested any new knowledge so it's answering based on the data it has from the training of the model itself. The ingestion is not that hard - let's get going!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
      "metadata": {},
      "source": [
        "## 3. Ingesting and retrieving knowledge\n",
        "The last step of this guided lab exercise is to ingest and retrieve knowledge together, so we finally do the RAG bit of this whole exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa216e8f",
      "metadata": {},
      "source": [
        "Let's start by asking the agent a question it does not really have knowledge about (it's a weird one but it will make sense later):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8922641d",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_input = \"What is the size of my pretzels?\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"type\": \"message\",\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"input_text\", \"text\": user_input}],\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\" Assistant:\", end=\" \", flush=True)\n",
        "\n",
        "event_logger = AgentEventLogger()\n",
        "\n",
        "for chunk in agent.create_turn(messages=messages, session_id=session_id, stream=True):\n",
        "    for log_msg in event_logger.log([chunk]):\n",
        "        print(log_msg, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8316e1da",
      "metadata": {},
      "source": [
        "As you can see, it does not know anything about any sizing of pretzels - this is good! Well, at least for us to ingest that information now!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c46b1487",
      "metadata": {},
      "source": [
        "Let's create a document based on a small, raw text sample, wrap it as a file and ingest it into our vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97a1851",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example text we wrap into a document to ingest into our vector store for later retrieval\n",
        "#\n",
        "# (I just love pretzels, don't judge me)\n",
        "raw_text = \"\"\"\n",
        "Size of my pretzels are 30cm.\n",
        "\"\"\"\n",
        "\n",
        "# Create a pseudo file object from the raw text as the API expects a file-like object\n",
        "pseudo_file = io.BytesIO(str(raw_text).encode('utf-8'))\n",
        "file_id = client.files.create(file=('raw_text.txt', pseudo_file, \"text/plain\"), purpose=\"assistants\").id\n",
        "\n",
        "# Ingest the file into the vector store\n",
        "vector_store_file = client.vector_stores.files.create(vector_store_id=vector_store.id, file_id=file_id)\n",
        "\n",
        "print(vector_store_file.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a193d890",
      "metadata": {},
      "source": [
        "Now let's ask it again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e67200",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_input = \"What is the size of my pretzels?\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"type\": \"message\",\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"input_text\", \"text\": user_input}],\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\" Assistant:\", end=\" \", flush=True)\n",
        "\n",
        "event_logger = AgentEventLogger()\n",
        "\n",
        "for chunk in agent.create_turn(messages=messages, session_id=session_id, stream=True):\n",
        "    for log_msg in event_logger.log([chunk]):\n",
        "        print(log_msg, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d94dc6",
      "metadata": {},
      "source": [
        "And that's how easy it is - with the configuration we have done previously and the capability of ingesting documents, we are able to augment the knowledge of the LLM by practically anything we want."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60cca291",
      "metadata": {},
      "source": [
        "Here an example of ingesting the content of a website:\n",
        "\n",
        "*NOTE: the lab environment does not have a GPU, embedding is done locally with the CPU and will take some time!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3729c3b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Take a look at the page itself, it mainly contains text so it's a good candidate for ingestion\n",
        "url = \"https://www.paulgraham.com/greatwork.html\"\n",
        "\n",
        "# We basically just fetch the content and wrap it as a file-like object for ingestion\n",
        "# just like we did with the raw text before\n",
        "response = requests.get(url)\n",
        "pseudo_file = io.BytesIO(str(response.content).encode('utf-8'))\n",
        "file_id = client.files.create(file=('paulgraham-greatwork.html', pseudo_file, \"text/html\"), purpose=\"assistants\").id\n",
        "\n",
        "# Ingest the file into the vector store\n",
        "vector_store_file = client.vector_stores.files.create(vector_store_id=vector_store.id, file_id=file_id)\n",
        "\n",
        "print(vector_store_file.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ba1c9a",
      "metadata": {},
      "source": [
        "Now let's see what output we will get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf6adba",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_input = \"Is there a file with content about Paul Graham in the knowledge base?\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"type\": \"message\",\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"input_text\", \"text\": user_input}],\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\" Assistant:\", end=\" \", flush=True)\n",
        "\n",
        "event_logger = AgentEventLogger()\n",
        "\n",
        "for chunk in agent.create_turn(messages=messages, session_id=session_id, stream=True):\n",
        "    for log_msg in event_logger.log([chunk]):\n",
        "        print(log_msg, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70161166",
      "metadata": {},
      "source": [
        "### Status\n",
        "Well done! By now you should have an environment capable of chatting with an agent and expanding it's knowledge by ingesting any kind of content you want. You need some base configuration and snippets for the ingestion part but once this is done, building on top of that is a piece of cake - that's how RAG is supposed to be :)\n",
        "\n",
        "Since the Llama Stack is part of OpenShift AI, getting this environment set up like this is very straight forward. We only have to enable the Llama Stack as part of the OpenShift AI operator and you are already able to build notebooks like this. Any Llama Stack client is feasible so you can build any application you need based on this stack!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd26201",
      "metadata": {},
      "source": [
        "## (Optional) Sandbox time!\n",
        "If you made it this far and you still have some time, feel free to use this part of the notebook to add your own code based on the snippets above to play around!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6462c9d0",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d77ce7",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ffe0adcf",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
        "\n",
        "I hope you learned something and enjoyed this lab! If you have any questions or want to try it out in your environment, feel free to get in touch with a Red Hatter of your choice! :)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
